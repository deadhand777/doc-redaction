{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Document Redaction Automation Service         The purpose of this project is to provide a service that automatically detects and redacts confidential information from documents.    <p> Repository     \u25c6 Documentation     \u25c6 Discussions </p>"},{"location":"#project-structure","title":"Project Structure","text":"<p>The project is organized as follows:</p> <p></p> Path Type Description Root Directory <code>pyproject.toml</code> Config Project configuration, dependencies, and build settings <code>Makefile</code> Build Development automation commands and build scripts <code>README.md</code> Doc Main project documentation and overview <code>uv.lock</code> Lock UV package manager lock file for reproducible builds /data/ Directory Document processing data storage <code>/data/contract/</code> Directory Original PDF contract files for processing <code>/data/markdown/</code> Directory Converted markdown files from PDF conversion <code>/data/confidential/</code> Directory JSON files containing detected sensitive data <code>/data/redact/</code> Directory Final redacted document outputs /src/doc_redaction/ Directory Main application source code <code>/src/doc_redaction/__init__.py</code> Module Package initialization file <code>/src/doc_redaction/agent.py</code> Module AI agent creation and configuration <code>/src/doc_redaction/output.py</code> Module Pydantic models for structured outputs (SensitiveData) <code>/src/doc_redaction/promt.py</code> Module System prompts for different AI agents <code>/src/doc_redaction/utils.py</code> Module Utility functions and custom exceptions <code>/src/doc_redaction/workflow.py</code> Module Main document processing workflow orchestration /src/doc_redaction/tool/ Directory Processing tools and utilities <code>/src/doc_redaction/tool/__init__.py</code> Module Tool package initialization <code>/src/doc_redaction/tool/detect_sensitive_data.py</code> Tool Sensitive information detection using regex patterns <code>/src/doc_redaction/tool/redact_sensitive_data.py</code> Tool Document redaction and content sanitization <code>/src/doc_redaction/tool/tool_utils.py</code> Tool Utility functions for tool operations <code>/src/doc_redaction/tool/document_processing.py</code> Tool Large document processing and chunking system /tests/ Directory Test suite for all application components <code>/tests/test_*.py</code> Tests Comprehensive unit and integration tests"},{"location":"#tooling","title":"Tooling","text":"<p>The templete utilizes the following tools for development, testing, and deployment:</p> <ul> <li>uv for dependency management</li> <li>CI/CD with GitHub Actions</li> <li>Pre-commit hooks with pre-commit</li> <li>Code quality with ruff, ty and prettier</li> <li>Documentation with MkDocs</li> <li>Compatibility testing for multiple versions of Python with tox-uv</li> <li>Containerization with Docker</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>See Getting Started</p>"},{"location":"#architecture","title":"Architecture","text":"<p>See Architecture</p>"},{"location":"#modules-and-features","title":"Modules and Features","text":"<p>See Modules documentation.</p>"},{"location":"agentic_workflow/","title":"Agent Workflow","text":"<p>The service is a agentic system composed of a graph-based multi-agent workflow.</p> <p>The document processing pipeline uses a Graph Multi-Agent Pattern to orchestrate three specialized agents that convert, analyze, and redact sensitive information from PDF contracts using AI agents powered by AWS Bedrock models. This graph-based approach enables parallel processing and efficient resource utilization through the Strands framework's <code>GraphBuilder</code>.</p> <p></p>"},{"location":"agentic_workflow/#graph-node-1-pdf-to-markdown-conversion-convert_result","title":"Graph Node 1: PDF to Markdown Conversion (<code>convert_result</code>)","text":"<p>Entry Point Agent - Multimodal agent with AI vision capabilities:</p> <ul> <li>Reads and understands PDF content from converted PNG images</li> <li>Converts PDF structure and content to clean markdown format</li> <li>Preserves document structure, formatting, and content hierarchy</li> <li>Tools: <code>image_reader</code>, <code>merge_markdown_strings</code>, <code>save_file</code>, <code>remove_temp_files</code></li> </ul> <p>Output: Markdown file representing the original PDF document Graph Position: Entry point that triggers downstream agents</p>"},{"location":"agentic_workflow/#graph-node-2-sensitive-data-detection-detector_result","title":"Graph Node 2: Sensitive Data Detection (<code>detector_result</code>)","text":"<p>Detection Agent - Specialized for sensitive information identification:</p> <ul> <li>Dependencies: Requires completion of conversion agent (<code>convert_result</code>)</li> <li>Analyzes document content using structured output with SensitiveData model</li> <li>Identifies and extracts sensitive information including:</li> <li>Personal information (names, emails, phone numbers)</li> <li>Company details (names, addresses, registration numbers)</li> <li>Document metadata and analysis information</li> <li>Tools: <code>current_time</code>, <code>detect_sensitive_data</code>, <code>omit_empty_keys</code></li> <li>Model Configuration: Uses Haiku model for efficient processing</li> </ul> <p>Output: Structured JSON file with detected sensitive information Graph Position: Parallel processing node that can execute concurrently with redaction planning</p>"},{"location":"agentic_workflow/#graph-node-3-document-redaction-redact_result","title":"Graph Node 3: Document Redaction (<code>redact_result</code>)","text":"<p>Redaction Agent - Focused on content sanitization:</p> <ul> <li>Dependencies: Requires both conversion (<code>convert_result</code>) and detection (<code>detector_result</code>) completion</li> <li>Systematically redacts all sensitive information identified by the detection agent</li> <li>Preserves document structure and non-sensitive content</li> <li>Maintains document readability while removing confidential data</li> <li>Tools: <code>save_file</code>, <code>redact_sensitive_data</code></li> <li>Model Configuration: Uses Haiku model optimized for redaction tasks</li> </ul> <p>Output: Redacted markdown file with sensitive information removed Graph Position: Final processing node that combines results from both upstream agents</p>"},{"location":"agentic_workflow/#graph-based-agent-architecture","title":"Graph-Based Agent Architecture","text":"<p>The workflow implements a Graph Multi-Agent Pattern using the Strands framework's <code>GraphBuilder</code>:</p>"},{"location":"agentic_workflow/#graph-structure","title":"Graph Structure","text":"<ul> <li>Nodes: Three specialized agents (Conversion, Detection, Redaction)</li> <li>Edges: Define dependencies and data flow between agents</li> <li>Entry Point: Conversion agent serves as the workflow entry point</li> <li>Parallel Execution: Detection and redaction agents can process concurrently after conversion</li> </ul>"},{"location":"agentic_workflow/#agent-configuration","title":"Agent Configuration","text":"<p>Each agent is purpose-built with specific: - System prompts tailored to their specialized task - Curated tool sets for required operations - Model configurations optimized for their workload (e.g., Haiku model for detection and redaction)</p>"},{"location":"agentic_workflow/#workflow-execution","title":"Workflow Execution","text":"<pre><code>builder = GraphBuilder()\nbuilder.add_node(multimodal_agent, \"convert_result\")\nbuilder.add_node(detector_agent, \"detector_result\")\nbuilder.add_node(redact_agent, \"redact_result\")\n\n# Define dependencies\nbuilder.add_edge(\"convert_result\", \"detector_result\")\nbuilder.add_edge(\"convert_result\", \"redact_result\")\nbuilder.add_edge(\"detector_result\", \"redact_result\")\n\nbuilder.set_entry_point(\"convert_result\")\nbuilder.set_execution_timeout(300)\ngraph = builder.build()\n</code></pre> <p>This architecture provides: - Parallel Processing: Detection and redaction can process simultaneously where dependencies allow - Optimized Resource Usage: Multiple agents can work concurrently - Flexible Execution: Graph structure enables dynamic workflow adaptation - Robust Error Handling: Built-in timeout and status management</p>"},{"location":"agentic_workflow/#graph-workflow-benefits","title":"Graph Workflow Benefits","text":"<p>The Graph Multi-Agent Pattern provides several key advantages over traditional linear workflows:</p>"},{"location":"agentic_workflow/#performance-enhancements","title":"Performance Enhancements","text":"<ul> <li>Parallel Execution: Detection and initial redaction planning can start simultaneously after conversion</li> <li>Resource Optimization: Multiple agents utilize available compute resources concurrently</li> <li>Reduced Latency: Overall processing time decreased through concurrent operations</li> </ul>"},{"location":"agentic_workflow/#scalability-flexibility","title":"Scalability &amp; Flexibility","text":"<ul> <li>Dynamic Adaptation: Graph structure allows for easy addition of new processing nodes</li> <li>Conditional Routing: Future enhancements can implement branching logic based on document characteristics</li> <li>Modular Design: Individual agents can be updated or replaced without affecting the entire workflow</li> </ul>"},{"location":"agentic_workflow/#reliability-monitoring","title":"Reliability &amp; Monitoring","text":"<ul> <li>Execution Status: Built-in workflow status tracking and timeout management</li> <li>Granular Metrics: Individual agent performance metrics for detailed analysis</li> <li>Error Isolation: Failures in one agent don't immediately cascade to others</li> </ul>"},{"location":"agentic_workflow/#output-artifacts","title":"Output Artifacts","text":"<ul> <li>Converted Document: Clean markdown representation of the original PDF</li> <li>Sensitive Data Catalog: Structured JSON with all detected sensitive information</li> <li>Redacted Document: Sanitized version safe for broader distribution</li> <li>Process Metrics: Detailed logging, token usage, and execution statistics for each agent</li> <li>Workflow Status: Graph execution status and accumulated usage metrics</li> </ul> <p>This graph-based workflow enables automated, AI-powered document redaction with enhanced performance, parallel processing capabilities, and full traceability. The structured output and execution metrics make it suitable for compliance and audit requirements while providing superior efficiency through multi-agent coordination.</p>"},{"location":"agentic_workflow/#example-input-and-output","title":"Example Input and Output","text":"<p>Sample input and output artifacts can be found in the data directory of this repository.</p> Raw Contract Converted Document Sensitive Data Catalog Redacted Document Raw spielbank_rocketbase_dienstleistungsvertrag Converted spielbank_rocketbase_dienstleistungsvertrag Extracted spielbank_rocketbase_dienstleistungsvertrag Redacted spielbank_rocketbase_dienstleistungsvertrag Raw rocketbase_aws_agreement Converted rocketbase_aws_agreement Extracted rocketbase_aws_agreement Redacted rocketbase_aws_agreement Raw spielbank_rocketbase_vertrag Converted spielbank_rocketbase_vertrag Extracted spielbank_rocketbase_vertrag Redacted spielbank_rocketbase_vertrag"},{"location":"architecture/","title":"Architecture","text":"<p>TODO</p>"},{"location":"getting_started/","title":"Getting started","text":"<p>Here's how to set up <code>doc-redaction</code> for local development. Please note this documentation assumes you already have <code>uv</code> and <code>Git</code> installed and ready to go.</p> <ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/deadhand777/doc-redaction.git\n</code></pre> <ol> <li>Now install the environment. Navigate into the directory</li> </ol> <pre><code>cd doc-redaction\n</code></pre> <p>Then, install and activate the environment with:</p> <pre><code>uv sync\n</code></pre> <ol> <li>Install pre-commit to run linters/formatters at commit time:</li> </ol> <pre><code>uv run pre-commit install\n</code></pre> <p>Congratulations, now your setup is completed!</p> <ol> <li>Run the workflow on a sample PDF document:</li> </ol> <pre><code>uv run src/doc_redaction/workflow.py --key spielbank_rocketbase_vertrag\n</code></pre>"},{"location":"modules/","title":"Modules","text":""},{"location":"modules/#agent-modules","title":"Agent Modules","text":""},{"location":"modules/#create_agent","title":"create_agent","text":"<p>Create and return a configured Agent.</p> <ul> <li>system_prompt: Required non-empty instructional prompt.</li> <li>name: Optional agent name (default: \"Strands Agent\").</li> <li>model: Optional model. Falls back to default bedrock_model if None.</li> <li>tools: Optional iterable of tool specs/objects. Converted internally to a list.</li> <li>output_model: Optional structured output model for the agent.</li> </ul> <p>Raises:</p> Type Description <code>MissingArgumentError</code> <p>If system_prompt is missing or empty.</p> <code>TypeError</code> <p>If system_prompt is not a string.</p> Source code in <code>src/doc_redaction/agent.py</code> <pre><code>def create_agent(\n    system_prompt: str,\n    name: str | None = \"Strands Agent\",\n    model: BedrockModel | OllamaModel | None = bedrock_model,\n    tools: Sequence[str | dict[str, str] | Any] | None = None,\n    output_model: Any | None = None,\n) -&gt; Agent:\n    \"\"\"\n    Create and return a configured Agent.\n\n    - system_prompt: Required non-empty instructional prompt.\n    - name: Optional agent name (default: \"Strands Agent\").\n    - model: Optional model. Falls back to default bedrock_model if None.\n    - tools: Optional iterable of tool specs/objects. Converted internally to a list.\n    - output_model: Optional structured output model for the agent.\n\n    Raises:\n        MissingArgumentError: If system_prompt is missing or empty.\n        TypeError: If system_prompt is not a string.\n    \"\"\"\n    if not isinstance(system_prompt, str):\n        raise ParameterTypeError(\"system_prompt\", \"a string\")\n    if not system_prompt.strip():\n        raise MissingArgumentError(\"system_prompt\")\n\n    # Fallback to default model if none provided\n    if model is None:\n        model = bedrock_model\n\n    # Normalize tools to a list (avoid mutable default pitfalls)\n    tools_list: list[str | dict[str, str] | Any] | None = list(tools) if tools else None\n\n    agent: Agent = Agent(\n        model=model,\n        system_prompt=system_prompt.strip(),\n        tools=tools_list,\n        structured_output_model=output_model,\n    )\n\n    if type(model) is BedrockModel:\n        agent.name = name\n\n    logger.info(\n        \"Agent created successfully\",\n        extra={\n            \"agent_name\": name,\n            \"model_id\": getattr(model, \"model_id\", \"unknown\"),\n            \"tools_count\": len(tools_list) if tools_list else 0,\n        },\n    )\n\n    return agent\n</code></pre>"},{"location":"modules/#agentic-workflow-modules","title":"Agentic Workflow Modules","text":""},{"location":"modules/#run_doc_processing_wf","title":"run_doc_processing_wf","text":"<p>Run the document processing workflow for a given document key.</p> Source code in <code>src/doc_redaction/workflow.py</code> <pre><code>def run_doc_processing_wf(key: str = \"spielbank_rocketbase_vertrag\") -&gt; tuple[dict[str, Any], GraphResult, str]:\n    \"\"\"\n    Run the document processing workflow for a given document key.\n    \"\"\"\n    if not isinstance(key, str) or not key:\n        raise InvalidDocumentKeyError()\n\n    # Step 0: Assess document quality\n    DOC_QUALITY_IN: str = f\"{Dir.Data}{Prefix.CONTRACT}{key}{Format.PDF}\"\n    DOC_QUALITY_OUT: str = f\"{Dir.Data}{Prefix.QUALITY}{key}{Format.JSON}\"\n    doc_quality: dict[str, Any] = assess_doc_quality(\n        file_path=DOC_QUALITY_IN,\n        output_path=DOC_QUALITY_OUT,\n    )\n\n    # Step 1: Convert input contract from PDF to markdwon format using vision model Agent\n    multimodal_agent: Agent = create_agent(\n        name=\"multimodal_agent\",\n        system_prompt=CONVERTER_SYSTEM_PROMPT,\n        tools=[\n            image_reader,\n            merge_markdown_strings,\n            save_file,\n            remove_temp_files,\n        ],\n    )\n\n    CONVERT_IN: list[str] = pdf_to_png(\n        pdf_path=f\"{Dir.Data}{Prefix.CONTRACT}{key}{Format.PDF}\",\n        output_dir=f\"{Dir.Data}{Prefix.TEMP}\",\n    )\n    CONVERT_OUT: str = f\"{Dir.Data}{Prefix.MARKDOWN}{key}{Format.MD}\"\n\n    # Step 2: Detect sensitve information Agent\n    DETECT_OUT: str = f\"{Dir.Data}{Prefix.CONFIDENTIAL}{key}{Format.JSON}\"\n    detector_agent: Agent = create_agent(\n        name=\"detector_agent\",\n        system_prompt=DETECTION_SYSTEM_PROMPT,\n        tools=[\n            current_time,\n            detect_sensitive_data,\n            omit_empty_keys,\n        ],\n        output_model=SensitiveData,\n    )\n    detector_agent.model.update_config(model_id=MODEL_IDS[\"haiku\"])\n\n    # Step 3: Redact sensitive information Agent\n    REDACT_OUT: str = f\"{Dir.Data}{Prefix.REDACT}{key}{Format.MD}\"\n\n    redact_agent: Agent = create_agent(\n        name=\"redact_agent\",\n        system_prompt=REDACTED_SYSTEM_PROMPT,\n        tools=[save_file, redact_sensitive_data],\n    )\n    redact_agent.model.update_config(model_id=MODEL_IDS[\"haiku\"])\n\n    # Step 4: Build and run workflow graph\n    builder: GraphBuilder = GraphBuilder()\n\n    builder.add_node(multimodal_agent, \"convert_result\")\n    builder.add_node(detector_agent, \"detector_result\")\n    builder.add_node(redact_agent, \"redact_result\")\n\n    builder.add_edge(\"convert_result\", \"detector_result\")\n    builder.add_edge(\"convert_result\", \"redact_result\")\n    builder.add_edge(\"detector_result\", \"redact_result\")\n\n    builder.set_entry_point(\"convert_result\")\n    builder.set_execution_timeout(300)\n    graph: Graph = builder.build()\n\n    user_prompt: str = f\"\"\"\n    1. Convert the following list of images to a single markdown: {CONVERT_IN}. Save the result to {CONVERT_OUT}.\n    2. Detect sensitive data. Return the results as structured_output as defined in {SensitiveData} schema. Save the result to {DETECT_OUT}.\n    3. Redact all information provided in detector_result except for the document_analysis field. Save the result to {REDACT_OUT}.\n    \"\"\"\n\n    result: GraphResult = graph(user_prompt)\n\n    logger.info(f\"Workflow status: {result.status.value}\")\n    logger.info(f\"Total token usage: {result.accumulated_usage}\")\n\n    # Step 5: Summarize token usage\n    agents: list[Agent] = [multimodal_agent, detector_agent, redact_agent]\n    token_summary: str = process_and_summarize_tokens(agents, result)\n    TOKEN_SUMMARY_OUT: str = f\"{Dir.Data}{Prefix.TOKEN}{key}{Format.JSON}\"\n    save_as_json(data=token_summary, filename=TOKEN_SUMMARY_OUT)\n\n    return doc_quality, result, token_summary\n</code></pre>"},{"location":"modules/#process_and_summarize_tokens","title":"process_and_summarize_tokens","text":"<p>Process token usage for all agents and return a summarized token usage string.</p> <p>Parameters:</p> Name Type Description Default <code>agents</code> <code>list[Agent]</code> <p>List of Agent objects used in the workflow</p> required <code>result</code> <code>Any</code> <p>Graph execution result containing accumulated usage data</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>JSON string with summarized token usage across all agents</p> Source code in <code>src/doc_redaction/workflow.py</code> <pre><code>def process_and_summarize_tokens(agents: list[Agent], result: Any) -&gt; str:\n    \"\"\"\n    Process token usage for all agents and return a summarized token usage string.\n\n    Args:\n        agents: List of Agent objects used in the workflow\n        result: Graph execution result containing accumulated usage data\n\n    Returns:\n        str: JSON string with summarized token usage across all agents\n    \"\"\"\n\n    all_agents_tokens: dict[str, dict[str, Any]] = {\n        agent.model.get_config()[\"model_id\"]: token_usage(content=result.results[node_name].accumulated_usage, model=agent.model.get_config()[\"model_id\"])\n        for agent, node_name in [(agents[0], \"convert_result\"), (agents[1], \"detector_result\"), (agents[2], \"redact_result\")]\n    }\n\n    result: str = summarize_token_usage(all_agents_tokens)\n\n    return result\n</code></pre>"},{"location":"modules/#output-modules","title":"Output Modules","text":""},{"location":"modules/#sensitivedata","title":"SensitiveData","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents sensitive data detected in a document.</p> Source code in <code>src/doc_redaction/output.py</code> <pre><code>class SensitiveData(BaseModel):\n    \"\"\"Represents sensitive data detected in a document.\"\"\"\n\n    document_analysis: DocumentAnalysis = Field(..., description=\"Metadata about the analyzed document.\")\n    parties: list[Party] = Field(..., description=\"Information about the parties involved in the contract.\")\n    representative: list[Representative] = Field(..., description=\"Information about the representatives of the parties.\")\n    contract_terms: ContractTerms = Field(..., description=\"Key terms and conditions of the contract.\")\n    risk_assessment: RiskAssessment = Field(..., description=\"Risk assessment of the contract.\")\n    data_protection_compliance: DataProtectionCompliance = Field(..., description=\"Data protection compliance details.\")\n</code></pre>"},{"location":"modules/#tool-modules","title":"Tool Modules","text":""},{"location":"modules/#detect_sensitive_data","title":"detect_sensitive_data","text":"<p>Tool for detecting sensitive data in markdown documents.</p>"},{"location":"modules/#tool.detect_sensitive_data.detect_sensitive_data","title":"<code>detect_sensitive_data(markdown_content)</code>","text":"<p>Detects and extracts sensitive information from markdown documents.</p> Source code in <code>src/doc_redaction/tool/detect_sensitive_data.py</code> <pre><code>@tool\ndef detect_sensitive_data(markdown_content: str) -&gt; dict[str, list[str]]:\n    \"\"\"Detects and extracts sensitive information from markdown documents.\"\"\"\n\n    text = remove_markdown_formatting(markdown_content)\n    results: dict[str, list[str]] = {}\n\n    # Mapping of result key -&gt; regex list or single regex\n    pattern_mapping = {\n        \"email_addresses\": [EMAIL_RE],\n        \"phone_numbers\": PHONE_REGEXES,\n        \"credit_card_numbers\": [CC_RE],\n        \"iban_numbers\": [IBAN_RE],\n        \"account_numbers\": [ACCOUNT_RE],\n        \"addresses\": ADDRESS_REGEXES,\n        \"people_names\": [NAME_RE],\n        \"currency_amounts\": CURRENCY_REGEXES,\n        \"percentages\": PERCENTAGE_REGEXES,\n        \"numbers\": NUMBER_REGEXES,\n    }\n\n    for key, regexes in pattern_mapping.items():\n        matches = set()\n        for regex in regexes:\n            for match in regex.findall(text):\n                # Additional filters for some types\n                if key == \"phone_numbers\" and len(re.sub(r\"[^\\d]\", \"\", match)) &lt; 7:\n                    continue\n                if key == \"credit_card_numbers\":\n                    digits = re.sub(r\"[^\\d]\", \"\", match)\n                    if not (13 &lt;= len(digits) &lt;= 19):\n                        continue\n                if key == \"people_names\" and match in COMMON_NON_NAMES:\n                    continue\n                matches.add(match)\n        if matches:\n            results[key] = list(matches)\n\n    # Add German &amp; English number words\n    numbers_set = set(results.get(\"numbers\", []))\n    for word in text.lower().split():\n        clean_word = word.strip(\".,;:!?\")\n        if clean_word in GERMAN_NUMBER_WORDS or clean_word in ENGLISH_NUMBER_WORDS:\n            numbers_set.add(clean_word)\n    if numbers_set:\n        results[\"numbers\"] = list(numbers_set)\n\n    return results\n</code></pre>"},{"location":"modules/#tool.detect_sensitive_data.remove_markdown_formatting","title":"<code>remove_markdown_formatting(markdown_text)</code>","text":"<p>Remove markdown formatting for cleaner analysis.</p> Source code in <code>src/doc_redaction/tool/detect_sensitive_data.py</code> <pre><code>def remove_markdown_formatting(markdown_text: str) -&gt; str:\n    \"\"\"Remove markdown formatting for cleaner analysis.\"\"\"\n    patterns = [\n        (r\"^#{1,6}\\s+\", \"\"),  # headers\n        (r\"\\*\\*(.+?)\\*\\*\", r\"\\1\"),  # bold\n        (r\"\\*(.+?)\\*\", r\"\\1\"),  # italic\n        (r\"__(.+?)__\", r\"\\1\"),  # underline\n        (r\"_(.+?)_\", r\"\\1\"),\n        (r\"`(.+?)`\", r\"\\1\"),  # inline code\n        (r\"```.*?```\", \"\", re.DOTALL),  # code blocks\n        (r\"\\[(.+?)\\]\\(.+?\\)\", r\"\\1\"),  # links\n        (r\"!\\[.*?\\]\\(.+?\\)\", \"\"),  # images\n        (r\"^---+$\", \"\", re.MULTILINE),  # hr\n        (r\"^\\s*[-*+]\\s+\", \"\", re.MULTILINE),  # unordered lists\n        (r\"^\\s*\\d+\\.\\s+\", \"\", re.MULTILINE),  # ordered lists\n        (r\"^&gt;\\s+\", \"\", re.MULTILINE),  # blockquotes\n    ]\n    text = markdown_text\n\n    for pat in patterns:\n        text = (\n            re.sub(pat[0], pat[1], text)  # if len(pat) == 2 else re.sub(pat[0], pat[1], text)  # flags=pat[2]\n        )\n\n    text = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", text)  # normalize spacing\n    return re.sub(r\"[ \\t]+\", \" \", text).strip()\n</code></pre>"},{"location":"modules/#redact_sensitive_data","title":"redact_sensitive_data","text":"<p>Tool for redacting sensitive information from markdown documents.</p>"},{"location":"modules/#tool.redact_sensitive_data.apply_redactions","title":"<code>apply_redactions(content, rules, redaction_symbol, preserve_structure)</code>","text":"<p>Apply redaction rules to the content based on user specifications.</p> Source code in <code>src/doc_redaction/tool/redact_sensitive_data.py</code> <pre><code>def apply_redactions(content: str, rules: str, redaction_symbol: str, preserve_structure: bool) -&gt; str:\n    \"\"\"\n    Apply redaction rules to the content based on user specifications.\n    \"\"\"\n    rules_lower = rules.lower()\n\n    # Common patterns\n    patterns = {\n        \"email\": (\n            [r\"email\", r\"e-mail\", \"@\"],\n            r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\",\n        ),\n        \"phone\": (\n            [r\"phone\", r\"telephone\", r\"number\"],\n            r\"(\\+?1[-.\\s]?)?\\(?([0-9]{3})\\)?[-.\\s]?([0-9]{3})[-.\\s]?([0-9]{4})\",\n        ),\n        \"ssn\": (\n            [r\"ssn\", r\"social security\", r\"social\"],\n            r\"\\b\\d{3}-?\\d{2}-?\\d{4}\\b\",\n        ),\n        \"credit_card\": (\n            [r\"credit card\", r\"card number\", r\"credit\"],\n            r\"\\b(?:\\d{4}[-\\s]?){3}\\d{4}\\b\",\n        ),\n        \"zip_code\": (\n            [r\"zip code\", r\"postal code\", r\"zip\"],\n            r\"\\b\\d{5}(?:-\\d{4})?\\b\",\n        ),\n        \"ip_address\": (\n            [r\"ip address\", r\"ip\"],\n            r\"\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b\",\n        ),\n        \"url\": (\n            [r\"url\", r\"link\", r\"website\"],\n            r'https?://[^\\s&lt;&gt;\"{}|\\\\^`\\[\\]]+',\n        ),\n        \"date\": (\n            [r\"date\", r\"birthday\", r\"birth\"],\n            r\"\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b|\\b\\d{4}[/-]\\d{1,2}[/-]\\d{1,2}\\b\",\n        ),\n        \"name\": (\n            [r\"name\", r\"person\", r\"individual\"],\n            r\"\\b[A-Z][a-z]+ [A-Z][a-z]+\\b\",\n        ),\n        \"address\": (\n            [r\"address\", r\"street\", r\"location\"],\n            r\"\\d+\\s+[A-Za-z\\s]+(?:Street|St|Avenue|Ave|Road|Rd|Drive|Dr|Lane|Ln|Boulevard|Blvd)\",\n        ),\n    }\n\n    def should_apply(keywords: list[str]) -&gt; bool:\n        return any(term in rules_lower for term in keywords)\n\n    redacted_content = content\n    for _, (keywords, pattern) in patterns.items():\n        if should_apply(keywords):\n            redacted_content = redact_pattern(redacted_content, pattern, redaction_symbol, preserve_structure)\n\n    # Handle custom terms\n    for term in extract_custom_terms(rules):\n        if term and len(term) &gt; 2:\n            pattern = re.escape(term)\n            redacted_content = redact_pattern(redacted_content, pattern, redaction_symbol, preserve_structure, case_insensitive=True)\n\n    return redacted_content\n</code></pre>"},{"location":"modules/#tool.redact_sensitive_data.extract_custom_terms","title":"<code>extract_custom_terms(rules)</code>","text":"<p>Extract potential custom terms to redact from the rules text. This is a simple approach - could be enhanced with NLP.</p> Source code in <code>src/doc_redaction/tool/redact_sensitive_data.py</code> <pre><code>def extract_custom_terms(rules: str) -&gt; list[str]:\n    \"\"\"\n    Extract potential custom terms to redact from the rules text.\n    This is a simple approach - could be enhanced with NLP.\n    \"\"\"\n    # Look for quoted terms or terms after \"redact\"\n    custom_terms = []\n\n    # Extract quoted terms\n    quoted_terms = re.findall(r\"['\\\"]([^'\\\"]+)['\\\"]\", rules)\n    custom_terms.extend(quoted_terms)\n\n    # Extract terms after \"redact\" or \"remove\"\n    redact_terms = re.findall(r\"(?:redact|remove|hide)\\s+(?:all\\s+)?([a-zA-Z\\s]+?)(?:\\s+(?:and|or|from)|$)\", rules, re.IGNORECASE)\n    for term in redact_terms:\n        term = term.strip()\n        if term and not any(common in term.lower() for common in [\"email\", \"phone\", \"number\", \"address\", \"name\", \"ssn\", \"credit\", \"card\"]):\n            custom_terms.append(term)\n\n    return custom_terms\n</code></pre>"},{"location":"modules/#tool.redact_sensitive_data.redact_pattern","title":"<code>redact_pattern(content, pattern, redaction_symbol, preserve_structure, case_insensitive=False)</code>","text":"<p>Redact matches of a specific pattern in the content.</p> Source code in <code>src/doc_redaction/tool/redact_sensitive_data.py</code> <pre><code>def redact_pattern(content: str, pattern: str, redaction_symbol: str, preserve_structure: bool, case_insensitive: bool = False) -&gt; str:\n    \"\"\"\n    Redact matches of a specific pattern in the content.\n    \"\"\"\n    flags = re.IGNORECASE if case_insensitive else 0\n\n    def replace_match(match):\n        matched_text = match.group(0)\n        if preserve_structure:\n            # Replace each character with a redaction character, preserving spaces and structure\n            redaction_char = \"\u2588\"\n            return re.sub(r\"\\S\", redaction_char, matched_text)\n        else:\n            return redaction_symbol\n\n    return re.sub(pattern, replace_match, content, flags=flags)\n</code></pre>"},{"location":"modules/#tool.redact_sensitive_data.redact_sensitive_data","title":"<code>redact_sensitive_data(tool, **kwargs)</code>","text":"<p>Redact sensitive information from markdown documents based on user specifications.</p> <p>Parameters:</p> Name Type Description Default <code>tool</code> <code>ToolUse</code> <p>The tool use object containing tool execution details</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to the tool</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ToolResult</code> <code>ToolResult</code> <p>The redacted markdown document</p> Source code in <code>src/doc_redaction/tool/redact_sensitive_data.py</code> <pre><code>def redact_sensitive_data(tool: ToolUse, **kwargs: Any) -&gt; ToolResult:\n    \"\"\"\n    Redact sensitive information from markdown documents based on user specifications.\n\n    Args:\n        tool: The tool use object containing tool execution details\n        **kwargs: Additional arguments passed to the tool\n\n    Returns:\n        ToolResult: The redacted markdown document\n    \"\"\"\n    try:\n        # Extract parameters\n        markdown_content = tool.get(\"input\", {}).get(\"markdown_content\", \"\")\n        redaction_rules = tool.get(\"input\", {}).get(\"redaction_rules\", \"\")\n        redaction_symbol = tool.get(\"input\", {}).get(\"redaction_symbol\", \"[REDACTED]\")\n        preserve_structure = tool.get(\"input\", {}).get(\"preserve_structure\", False)\n\n        if not markdown_content:\n            return {\n                \"toolUseId\": tool[\"toolUseId\"],\n                \"status\": \"error\",\n                \"content\": [{\"text\": \"Error: No markdown content provided\"}],\n            }\n\n        if not redaction_rules:\n            return {\n                \"toolUseId\": tool[\"toolUseId\"],\n                \"status\": \"error\",\n                \"content\": [{\"text\": \"Error: No redaction rules specified\"}],\n            }\n\n        # Parse redaction rules and apply redactions\n        redacted_content = apply_redactions(markdown_content, redaction_rules, redaction_symbol, preserve_structure)\n\n        return {\n            \"toolUseId\": tool[\"toolUseId\"],\n            \"status\": \"success\",\n            \"content\": [{\"text\": f\"Successfully redacted sensitive information based on rules: '{redaction_rules}'\\n\\nRedacted markdown document:\\n\\n{redacted_content}\"}],\n        }\n\n    except Exception as e:\n        return {\n            \"toolUseId\": tool[\"toolUseId\"],\n            \"status\": \"error\",\n            \"content\": [{\"text\": f\"Error processing redaction: {e!s}\"}],\n        }\n</code></pre>"},{"location":"modules/#data-storage","title":"Data Storage","text":"<p>Write a JSON-formatted string to a file and log the operation.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>JSON string content to write.</p> required <code>filename</code> <code>str</code> <p>Destination file path. Existing content will be overwritten.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If the file cannot be opened or written.</p> Logs <p>INFO: On successful save with the target file path.</p> Example <p>save_as_json(res, \"data/confidential/rocketbase_aws_agreement_sensitive_structures_v4.json\")</p> Source code in <code>src/doc_redaction/tool/store_data.py</code> <pre><code>@tool\ndef save_as_json(data: str, filename: str) -&gt; None:\n    \"\"\"Write a JSON-formatted string to a file and log the operation.\n\n    Parameters:\n        data: JSON string content to write.\n        filename: Destination file path. Existing content will be overwritten.\n\n    Returns:\n        None\n\n    Raises:\n        OSError: If the file cannot be opened or written.\n\n    Logs:\n        INFO: On successful save with the target file path.\n\n    Example:\n        save_as_json(res, \"data/confidential/rocketbase_aws_agreement_sensitive_structures_v4.json\")\n    \"\"\"\n    with open(filename, \"w\") as f:\n        f.write(data)\n    logger.info(f\"Saved structured output to {filename}\")\n</code></pre>"},{"location":"modules/#other-tools","title":"Other Tools","text":"<p>Write a formatted string to a file and log the operation.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>String content to write.</p> required <code>filename</code> <code>str</code> <p>Destination file path. Existing content will be overwritten.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If the file cannot be opened or written.</p> Logs <p>INFO: On successful save with the target file path.</p> Example <p>save_file(res, \"data/confidential/rocketbase_aws_agreement_sensitive_structures.json\")</p> Source code in <code>src/doc_redaction/tool/tool_utils.py</code> <pre><code>@tool\ndef save_file(data: str, filename: str) -&gt; None:\n    \"\"\"Write a formatted string to a file and log the operation.\n\n    Parameters:\n        data: String content to write.\n        filename: Destination file path. Existing content will be overwritten.\n\n    Returns:\n        None\n\n    Raises:\n        OSError: If the file cannot be opened or written.\n\n    Logs:\n        INFO: On successful save with the target file path.\n\n    Example:\n        save_file(res, \"data/confidential/rocketbase_aws_agreement_sensitive_structures.json\")\n    \"\"\"\n    output_dir = Path(filename).parent\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    with open(filename, \"w\") as f:\n        f.write(data)\n    logger.info(f\"Saved structured output to {filename}\")\n</code></pre> <p>Remove temporary files in the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Directory path to remove.</p> <code>'data/temp/'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/doc_redaction/tool/tool_utils.py</code> <pre><code>@tool\ndef remove_temp_files(path: str = \"data/temp/\") -&gt; None:\n    \"\"\"\n    Remove temporary files in the specified directory.\n\n    Parameters:\n        path: Directory path to remove.\n\n    Returns:\n        None\n    \"\"\"\n\n    shutil.rmtree(path)\n    logger.info(f\"Removed temporary files in directory: {path}\")\n</code></pre> <p>Parse a JSON object string and return only the items with non-empty values.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>A JSON-encoded object mapping string keys to lists of strings.</p> required <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>A dict[str, list[str]] containing only entries whose values are truthy (e.g., non-empty lists).</p> <p>Raises:</p> Type Description <code>JSONDecodeError</code> <p>If the input is not valid JSON.</p> Source code in <code>src/doc_redaction/tool/tool_utils.py</code> <pre><code>@tool\ndef omit_empty_keys(s: str) -&gt; dict[str, list[str]]:\n    \"\"\"\n    Parse a JSON object string and return only the items with non-empty values.\n\n    Args:\n        s: A JSON-encoded object mapping string keys to lists of strings.\n\n    Returns:\n        A dict[str, list[str]] containing only entries whose values are truthy (e.g., non-empty lists).\n\n    Raises:\n        json.JSONDecodeError: If the input is not valid JSON.\n    \"\"\"\n    return {key: value for key, value in json.loads(s).items() if value}\n</code></pre>"},{"location":"modules/#utility-modules","title":"Utility Modules","text":""},{"location":"modules/#this-section-includes-common-utility-functions","title":"This section includes common utility functions.","text":""},{"location":"modules/#save_as_json","title":"save_as_json","text":"<p>Write a JSON-formatted string to a file and log the operation.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>JSON string content to write.</p> required <code>filename</code> <code>str</code> <p>Destination file path. Existing content will be overwritten.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If the file cannot be opened or written.</p> Logs <p>INFO: On successful save with the target file path.</p> Example <p>save_as_json(res, \"data/confidential/rocketbase_aws_agreement_sensitive_structures.json\")</p> Source code in <code>src/doc_redaction/utils/commons.py</code> <pre><code>def save_as_json(data: str, filename: str) -&gt; None:\n    \"\"\"Write a JSON-formatted string to a file and log the operation.\n\n    Parameters:\n        data: JSON string content to write.\n        filename: Destination file path. Existing content will be overwritten.\n\n    Returns:\n        None\n\n    Raises:\n        OSError: If the file cannot be opened or written.\n\n    Logs:\n        INFO: On successful save with the target file path.\n\n    Example:\n        save_as_json(res, \"data/confidential/rocketbase_aws_agreement_sensitive_structures.json\")\n    \"\"\"\n    output_dir = Path(filename).parent\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    with open(filename, \"w\") as f:\n        f.write(data)\n    logger.info(f\"Saved structured output to {filename}\")\n</code></pre>"},{"location":"modules/#get_pdf_page_count","title":"get_pdf_page_count","text":"<p>Return the number of pages in a PDF file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the local PDF file.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Total number of pages in the PDF.</p> <p>Raises:</p> Type Description <code>PDFProcessingError</code> <p>If the file cannot be opened, is not a valid PDF, or the page count cannot be determined.</p> Example <p>page_count = get_pdf_page_count(\"path/to/file.pdf\") print(f\"The document has {page_count} pages.\")</p> Source code in <code>src/doc_redaction/utils/commons.py</code> <pre><code>def get_pdf_page_count(file_path: str) -&gt; int:\n    \"\"\"\n    Return the number of pages in a PDF file.\n\n    Parameters:\n        file_path (str): Path to the local PDF file.\n\n    Returns:\n        int: Total number of pages in the PDF.\n\n    Raises:\n        PDFProcessingError: If the file cannot be opened, is not a valid PDF, or the page count cannot be determined.\n\n    Example:\n        page_count = get_pdf_page_count(\"path/to/file.pdf\")\n        print(f\"The document has {page_count} pages.\")\n    \"\"\"\n    try:\n        with open(file_path, \"rb\") as file:\n            pdf_reader = pypdf.PdfReader(file)\n            return len(pdf_reader.pages)\n    except Exception as e:\n        raise PDFProcessingError(file_path, e) from e\n</code></pre>"},{"location":"modules/#get_file_size","title":"get_file_size","text":"<p>Return the size of a file in bytes.</p> <pre><code>Parameters:\n    file_path (str): Path to the file.\n\nReturns:\n    int: File size in bytes.\n\nRaises:\n    FileNotFoundError: If the file does not exist.\n    OSError: If the size cannot be retrieved due to an OS-related error.\n\nExample:\n    size = get_file_size(\"/path/to/file.txt\")\n    print(f\"File size: {size} bytes\")\n</code></pre> Source code in <code>src/doc_redaction/utils/commons.py</code> <pre><code>def get_file_size(file_path: str) -&gt; int:\n    \"\"\"\n    Return the size of a file in bytes.\n\n        Parameters:\n            file_path (str): Path to the file.\n\n        Returns:\n            int: File size in bytes.\n\n        Raises:\n            FileNotFoundError: If the file does not exist.\n            OSError: If the size cannot be retrieved due to an OS-related error.\n\n        Example:\n            size = get_file_size(\"/path/to/file.txt\")\n            print(f\"File size: {size} bytes\")\n    \"\"\"\n    return os.path.getsize(file_path)\n</code></pre>"},{"location":"modules/#invalidcontenttype","title":"InvalidContentType","text":"<p>               Bases: <code>TypeError</code></p> <p>Raised when content is neither str nor int.</p> Source code in <code>src/doc_redaction/utils/commons.py</code> <pre><code>class InvalidContentType(TypeError):\n    \"\"\"Raised when content is neither str nor int.\"\"\"\n\n    def __init__(self, actual_type: type):\n        super().__init__(f\"Content must be str or int, got {actual_type.__name__}\")\n</code></pre>"},{"location":"modules/#invaliddocumentkeyerror","title":"InvalidDocumentKeyError","text":"<p>               Bases: <code>ValueError</code></p> <p>Raised when the provided document key is missing or invalid.</p> Source code in <code>src/doc_redaction/utils/commons.py</code> <pre><code>class InvalidDocumentKeyError(ValueError):\n    \"\"\"Raised when the provided document key is missing or invalid.\"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__(\"A document key must be provided as a non-empty string.\")\n</code></pre>"},{"location":"modules/#missingargumenterror","title":"MissingArgumentError","text":"<p>               Bases: <code>ValueError</code></p> <p>Raised when a required argument is missing.</p> Source code in <code>src/doc_redaction/utils/commons.py</code> <pre><code>class MissingArgumentError(ValueError):\n    \"\"\"Raised when a required argument is missing.\"\"\"\n\n    def __init__(self, argument_name: str) -&gt; None:\n        super().__init__(f\"{argument_name} must be provided\")\n</code></pre>"},{"location":"modules/#pdfprocessingerror","title":"PDFProcessingError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when PDF processing fails.</p> Source code in <code>src/doc_redaction/utils/commons.py</code> <pre><code>class PDFProcessingError(Exception):\n    \"\"\"Raised when PDF processing fails.\"\"\"\n\n    def __init__(self, file_path: str, e: Exception) -&gt; None:\n        super().__init__(f\"Could not determine page count for {file_path}: {e}\")\n        self.file_path = file_path\n</code></pre>"}]}